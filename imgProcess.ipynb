{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rice grain quality recognition project\n",
    "1.Build data structure for Mask_RCNN\n",
    "    - setup training data heiarchy\n",
    "    - parse and load image and annotation file\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras==2.1.0; pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mrcnn import visualize\n",
    "from mrcnn.utils import Dataset, extract_bboxes, resize_image, resize_mask\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN, mold_image, load_image_gt, log\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_dir = os.getcwd() + '/training/'\n",
    "test_dir = os.getcwd() + '/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_boxes(filename):\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        ann = json.load(f)\n",
    "        \n",
    "    height = ann['imageHeight']\n",
    "    width = ann['imageWidth']\n",
    "    all_boxes = ann['shapes']\n",
    "    \n",
    "    boxes = [(shape['label'], \n",
    "            np.array(shape['points'], \n",
    "            dtype=np.int16)) for shape in all_boxes]\n",
    "    return height, width, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class riceDataset(Dataset):\n",
    "    def load_dataset(self, dataset_dir, select_labels):\n",
    "        \n",
    "        images_dir = dataset_dir + 'imgs/'\n",
    "        annots_dir = dataset_dir + 'annots/'\n",
    "        \n",
    "        with open(dataset_dir + 'train_labels.csv') as tl:\n",
    "            labels = re.split('[,\\n]', tl.read()[:-1])\n",
    "            print('labels: ', labels)\n",
    "        \n",
    "        for i, label in zip(range(len(labels)), labels):\n",
    "            if label in select_labels:\n",
    "                self.add_class('dataset', i, label)\n",
    "        \n",
    "        for filename in os.listdir(images_dir):\n",
    "            for l in select_labels:\n",
    "                if l in filename:\n",
    "                    image_id = filename[:-4]\n",
    "                    image_path = images_dir + filename\n",
    "                    annots_path = annots_dir + image_id + '.json'\n",
    "\n",
    "                    height, width, boxes = extract_boxes(annots_path)\n",
    "                    self.add_image('dataset', \n",
    "                                   image_id=image_id, \n",
    "                                   path=image_path,\n",
    "                                   width=width,\n",
    "                                   height=height,\n",
    "                                   boxes=boxes,\n",
    "                                   annotation=annots_path)\n",
    "        \n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        info = self.image_info[image_id]\n",
    "        path = info['annotation']\n",
    "        masks = np.zeros([info['height'], \n",
    "                          info['width'], \n",
    "                          len(info['boxes'])], dtype='uint8')\n",
    "        \n",
    "        class_ids = []\n",
    "        for i in range(len(info['boxes'])):\n",
    "            box = info['boxes'][i]\n",
    "            row_s, row_e = box[1][0][1], box[1][1][1]\n",
    "            col_s, col_e = box[1][0][0], box[1][1][0]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            class_ids.append(self.class_names.index(box[0]))\n",
    "        \n",
    "        return masks, np.asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    def image_reference(self, image_id):\n",
    "        \n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiceConfig(Config):\n",
    "    BACKBONE = 'resnet50'\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    NAME = 'rice_config'\n",
    "    STEPS_PER_EPOCH = 1\n",
    "    NUM_CLASSES = 2\n",
    "    LEARNING_RATE = 0.001\n",
    "    BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              crop\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           rice_config\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = RiceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  ['bad', 'full', 'cracked', 'heat', 'white']\n",
      "labels:  ['bad', 'full', 'cracked', 'heat', 'white']\n",
      "Train: 1\n"
     ]
    }
   ],
   "source": [
    "train = riceDataset()\n",
    "train.load_dataset(train_dir, ['full'])\n",
    "train.prepare()\n",
    "\n",
    "test = riceDataset()\n",
    "test.load_dataset(test_dir, ['full'])\n",
    "test.prepare()\n",
    "print('Train: %d' % len(train.image_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_id = 1\n",
    "# image = train.load_image(image_id)\n",
    "# print(image.shape)\n",
    "\n",
    "# mask, class_ids = train.load_mask(image_id)\n",
    "# print(mask.shape)\n",
    "# plt.imshow(image)\n",
    "# plt.imshow(mask[:, :, 3], cmap='Reds', alpha=.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'full', 'source': 'dataset', 'path': '/Users/kyy/riceCV/training/imgs/full.jpg', 'width': 960, 'height': 1706, 'boxes': [('full', array([[465, 710],\n",
      "       [503, 743]], dtype=int16)), ('full', array([[541, 750],\n",
      "       [572, 792]], dtype=int16)), ('full', array([[596, 695],\n",
      "       [630, 739]], dtype=int16)), ('full', array([[672, 691],\n",
      "       [709, 730]], dtype=int16)), ('full', array([[519, 623],\n",
      "       [562, 654]], dtype=int16)), ('full', array([[440, 592],\n",
      "       [476, 638]], dtype=int16)), ('full', array([[334, 710],\n",
      "       [378, 740]], dtype=int16)), ('full', array([[427, 823],\n",
      "       [463, 857]], dtype=int16)), ('full', array([[494, 811],\n",
      "       [530, 850]], dtype=int16)), ('full', array([[503, 896],\n",
      "       [534, 937]], dtype=int16)), ('full', array([[604, 909],\n",
      "       [637, 951]], dtype=int16)), ('full', array([[705, 876],\n",
      "       [739, 911]], dtype=int16)), ('full', array([[699, 776],\n",
      "       [740, 809]], dtype=int16)), ('full', array([[768, 734],\n",
      "       [804, 767]], dtype=int16)), ('full', array([[775, 678],\n",
      "       [811, 711]], dtype=int16)), ('full', array([[707, 635],\n",
      "       [742, 673]], dtype=int16)), ('full', array([[660, 632],\n",
      "       [695, 671]], dtype=int16)), ('full', array([[580, 553],\n",
      "       [620, 591]], dtype=int16)), ('full', array([[688, 575],\n",
      "       [717, 613]], dtype=int16)), ('full', array([[768, 598],\n",
      "       [809, 639]], dtype=int16)), ('full', array([[790, 829],\n",
      "       [829, 864]], dtype=int16)), ('full', array([[788, 927],\n",
      "       [818, 969]], dtype=int16)), ('full', array([[703, 952],\n",
      "       [739, 994]], dtype=int16)), ('full', array([[ 715, 1011],\n",
      "       [ 759, 1042]], dtype=int16)), ('full', array([[ 780, 1014],\n",
      "       [ 820, 1050]], dtype=int16)), ('full', array([[ 451, 1003],\n",
      "       [ 487, 1044]], dtype=int16)), ('full', array([[386, 965],\n",
      "       [426, 998]], dtype=int16)), ('full', array([[419, 893],\n",
      "       [455, 932]], dtype=int16)), ('full', array([[332, 887],\n",
      "       [374, 924]], dtype=int16)), ('full', array([[316, 800],\n",
      "       [349, 840]], dtype=int16)), ('full', array([[302, 649],\n",
      "       [333, 692]], dtype=int16)), ('full', array([[280, 559],\n",
      "       [322, 594]], dtype=int16)), ('full', array([[376, 534],\n",
      "       [407, 575]], dtype=int16)), ('full', array([[ 592,  965],\n",
      "       [ 623, 1003]], dtype=int16)), ('full', array([[ 642, 1043],\n",
      "       [ 681, 1076]], dtype=int16)), ('full', array([[ 538, 1123],\n",
      "       [ 570, 1165]], dtype=int16)), ('full', array([[ 621, 1133],\n",
      "       [ 657, 1177]], dtype=int16)), ('full', array([[498, 445],\n",
      "       [524, 485]], dtype=int16)), ('full', array([[566, 444],\n",
      "       [604, 484]], dtype=int16)), ('full', array([[624, 496],\n",
      "       [665, 526]], dtype=int16)), ('full', array([[676, 495],\n",
      "       [718, 523]], dtype=int16)), ('full', array([[737, 441],\n",
      "       [765, 481]], dtype=int16)), ('full', array([[630, 394],\n",
      "       [659, 435]], dtype=int16)), ('full', array([[556, 380],\n",
      "       [587, 421]], dtype=int16)), ('full', array([[469, 378],\n",
      "       [511, 410]], dtype=int16)), ('full', array([[453, 415],\n",
      "       [486, 457]], dtype=int16)), ('full', array([[405, 453],\n",
      "       [447, 487]], dtype=int16)), ('full', array([[406, 495],\n",
      "       [444, 531]], dtype=int16)), ('full', array([[375, 380],\n",
      "       [412, 418]], dtype=int16)), ('full', array([[329, 442],\n",
      "       [364, 477]], dtype=int16)), ('full', array([[314, 488],\n",
      "       [352, 521]], dtype=int16)), ('full', array([[250, 465],\n",
      "       [289, 501]], dtype=int16)), ('full', array([[195, 520],\n",
      "       [236, 553]], dtype=int16)), ('full', array([[170, 570],\n",
      "       [206, 609]], dtype=int16)), ('full', array([[115, 581],\n",
      "       [159, 612]], dtype=int16)), ('full', array([[111, 647],\n",
      "       [149, 683]], dtype=int16)), ('full', array([[172, 661],\n",
      "       [203, 700]], dtype=int16)), ('full', array([[232, 704],\n",
      "       [265, 737]], dtype=int16)), ('full', array([[251, 651],\n",
      "       [282, 693]], dtype=int16)), ('full', array([[305, 752],\n",
      "       [337, 794]], dtype=int16)), ('full', array([[203, 751],\n",
      "       [237, 789]], dtype=int16)), ('full', array([[171, 741],\n",
      "       [202, 781]], dtype=int16)), ('full', array([[ 83, 705],\n",
      "       [124, 734]], dtype=int16)), ('full', array([[104, 764],\n",
      "       [139, 801]], dtype=int16)), ('full', array([[ 56, 802],\n",
      "       [ 92, 834]], dtype=int16)), ('full', array([[ 71, 890],\n",
      "       [105, 922]], dtype=int16)), ('full', array([[121, 829],\n",
      "       [164, 865]], dtype=int16)), ('full', array([[172, 865],\n",
      "       [201, 903]], dtype=int16)), ('full', array([[257, 865],\n",
      "       [287, 902]], dtype=int16)), ('full', array([[214, 944],\n",
      "       [250, 979]], dtype=int16)), ('full', array([[ 123,  989],\n",
      "       [ 159, 1024]], dtype=int16)), ('full', array([[ 146, 1037],\n",
      "       [ 183, 1071]], dtype=int16)), ('full', array([[ 219, 1115],\n",
      "       [ 253, 1157]], dtype=int16)), ('full', array([[ 258,  988],\n",
      "       [ 299, 1025]], dtype=int16)), ('full', array([[ 279, 1062],\n",
      "       [ 309, 1105]], dtype=int16)), ('full', array([[ 291, 1175],\n",
      "       [ 337, 1209]], dtype=int16)), ('full', array([[ 407, 1186],\n",
      "       [ 443, 1223]], dtype=int16)), ('full', array([[ 578, 1197],\n",
      "       [ 614, 1234]], dtype=int16)), ('full', array([[ 469, 1121],\n",
      "       [ 500, 1164]], dtype=int16)), ('full', array([[ 459, 1066],\n",
      "       [ 498, 1103]], dtype=int16)), ('full', array([[ 393, 1021],\n",
      "       [ 422, 1060]], dtype=int16)), ('full', array([[ 337, 1010],\n",
      "       [ 382, 1044]], dtype=int16)), ('full', array([[ 531, 1012],\n",
      "       [ 569, 1047]], dtype=int16)), ('full', array([[ 575, 1031],\n",
      "       [ 615, 1063]], dtype=int16)), ('full', array([[ 711, 1121],\n",
      "       [ 749, 1156]], dtype=int16)), ('full', array([[ 797, 1076],\n",
      "       [ 826, 1119]], dtype=int16)), ('full', array([[890, 934],\n",
      "       [922, 977]], dtype=int16)), ('full', array([[894, 862],\n",
      "       [928, 902]], dtype=int16)), ('full', array([[813, 786],\n",
      "       [842, 826]], dtype=int16)), ('full', array([[901, 786],\n",
      "       [937, 817]], dtype=int16)), ('full', array([[883, 686],\n",
      "       [913, 731]], dtype=int16)), ('full', array([[870, 648],\n",
      "       [903, 684]], dtype=int16)), ('full', array([[824, 629],\n",
      "       [866, 661]], dtype=int16)), ('full', array([[845, 571],\n",
      "       [892, 602]], dtype=int16)), ('full', array([[797, 518],\n",
      "       [837, 550]], dtype=int16)), ('full', array([[685, 544],\n",
      "       [725, 572]], dtype=int16)), ('full', array([[616, 584],\n",
      "       [654, 616]], dtype=int16)), ('full', array([[464, 554],\n",
      "       [503, 589]], dtype=int16)), ('full', array([[363, 603],\n",
      "       [397, 636]], dtype=int16)), ('full', array([[332, 592],\n",
      "       [357, 628]], dtype=int16))], 'annotation': '/Users/kyy/riceCV/training/annots/full.json'}\n"
     ]
    }
   ],
   "source": [
    "for image_id in train.image_ids:\n",
    "    info = train.image_info[image_id]\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_id = 0\n",
    "# # load the image\n",
    "# image = train.load_image(image_id)\n",
    "# # load the masks and the class ids\n",
    "# mask, class_ids = train.load_mask(image_id)\n",
    "# # extract bounding boxes from the masks\n",
    "# bbox = extract_bboxes(mask)\n",
    "# # display image with masks and bounding boxes\n",
    "# display_instances(image, bbox, mask, class_ids, train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit = 4\n",
    "# image_id = np.random.choice(train.image_ids, 1)[0]\n",
    "# ax = get_ax(rows=2, cols=limit//2)\n",
    "# for i in range(limit):\n",
    "#     image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "#         train, config, image_id, use_mini_mask=False)\n",
    "#     visualize.display_instances(image, bbox, mask, class_ids,\n",
    "#                                 train.class_names, ax=ax[i//2, i % 2],\n",
    "#                                 show_mask=False, show_bbox=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load random image and mask.\n",
    "# image_id = np.random.choice(train.image_ids, 1)[0]\n",
    "# image = train.load_image(image_id)\n",
    "# mask, class_ids = train.load_mask(image_id)\n",
    "# original_shape = image.shape\n",
    "# # Resize\n",
    "# image, window, scale, padding, _ = resize_image(\n",
    "#     image, \n",
    "#     min_dim=config.IMAGE_MIN_DIM, \n",
    "#     max_dim=config.IMAGE_MAX_DIM,\n",
    "#     mode=config.IMAGE_RESIZE_MODE)\n",
    "# mask = resize_mask(mask, scale, padding)\n",
    "# # Compute Bounding box\n",
    "# bbox = extract_bboxes(mask)\n",
    "\n",
    "# # Display image and additional stats\n",
    "# print(\"image_id: \", image_id, train.image_reference(image_id))\n",
    "# print(\"Original shape: \", original_shape)\n",
    "# log(\"image\", image)\n",
    "# log(\"mask\", mask)\n",
    "# log(\"class_ids\", class_ids)\n",
    "# log(\"bbox\", bbox)\n",
    "# # Display image and instances\n",
    "# visualize.display_instances(image, bbox, mask, class_ids, train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_id = 0\n",
    "# image = train.load_image(image_id)\n",
    "# mask, class_ids = train.load_mask(image_id)\n",
    "# original_shape = image.shape\n",
    "# # resize\n",
    "# image, window, scale, padding, _ = resize_image(image,\n",
    "#                                                 min_dim=config.IMAGE_MIN_DIM,\n",
    "#                                                 max_dim=config.IMAGE_MAX_DIM,\n",
    "#                                                 mode=config.IMAGE_RESIZE_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  256\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  256\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              crop\n",
      "IMAGE_SHAPE                    [256 256   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           rice_config\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: ./rice_config20200405T1509/mask_rcnn_rice_config_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 503s 503s/step - loss: 3.8459 - rpn_class_loss: 0.7084 - rpn_bbox_loss: 0.5566 - mrcnn_class_loss: 1.2031 - mrcnn_bbox_loss: 0.9910 - mrcnn_mask_loss: 0.3868 - val_loss: 3.2651 - val_rpn_class_loss: 0.1831 - val_rpn_bbox_loss: 0.5591 - val_mrcnn_class_loss: 1.0898 - val_mrcnn_bbox_loss: 1.0314 - val_mrcnn_mask_loss: 0.4018\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 215s 215s/step - loss: 3.1042 - rpn_class_loss: 0.5013 - rpn_bbox_loss: 0.4136 - mrcnn_class_loss: 0.9345 - mrcnn_bbox_loss: 0.9086 - mrcnn_mask_loss: 0.3461 - val_loss: 2.7902 - val_rpn_class_loss: 0.1576 - val_rpn_bbox_loss: 0.6456 - val_mrcnn_class_loss: 0.7941 - val_mrcnn_bbox_loss: 0.7968 - val_mrcnn_mask_loss: 0.3961\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 254s 254s/step - loss: 2.7323 - rpn_class_loss: 0.4434 - rpn_bbox_loss: 0.3639 - mrcnn_class_loss: 0.7555 - mrcnn_bbox_loss: 0.7910 - mrcnn_mask_loss: 0.3786 - val_loss: 2.2366 - val_rpn_class_loss: 0.1449 - val_rpn_bbox_loss: 0.6195 - val_mrcnn_class_loss: 0.5360 - val_mrcnn_bbox_loss: 0.5458 - val_mrcnn_mask_loss: 0.3903\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 244s 244s/step - loss: 2.0328 - rpn_class_loss: 0.3143 - rpn_bbox_loss: 0.3237 - mrcnn_class_loss: 0.5102 - mrcnn_bbox_loss: 0.4993 - mrcnn_mask_loss: 0.3854 - val_loss: 1.9891 - val_rpn_class_loss: 0.1394 - val_rpn_bbox_loss: 0.5706 - val_mrcnn_class_loss: 0.4251 - val_mrcnn_bbox_loss: 0.4680 - val_mrcnn_mask_loss: 0.3860\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 223s 223s/step - loss: 2.3729 - rpn_class_loss: 0.5256 - rpn_bbox_loss: 0.5047 - mrcnn_class_loss: 0.4530 - mrcnn_bbox_loss: 0.5238 - mrcnn_mask_loss: 0.3658 - val_loss: 1.9285 - val_rpn_class_loss: 0.1014 - val_rpn_bbox_loss: 0.5481 - val_mrcnn_class_loss: 0.3602 - val_mrcnn_bbox_loss: 0.5323 - val_mrcnn_mask_loss: 0.3865\n"
     ]
    }
   ],
   "source": [
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "model.load_weights('mask_rcnn_coco.h5', by_name=True, exclude=['mrcnn_class_logits', \n",
    "                                                                'mrcnn_bbox_fc', \n",
    "                                                                \"mrcnn_bbox\", \n",
    "                                                                \"mrcnn_mask\"])\n",
    "model.train(train, test, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rice_data = riceDataset()\n",
    "# h, w, data = extract_polygon('./training/annots/bad.json')\n",
    "# rice_data.load_dataset(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(dataset, model, cfg):\n",
    "# \tAPs = list()\n",
    "# \tfor image_id in dataset.image_ids:\n",
    "# \t\t# load image, bounding boxes and masks for the image id\n",
    "# \t\timage, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n",
    "# \t\t# convert pixel values (e.g. center)\n",
    "# \t\tscaled_image = mold_image(image, cfg)\n",
    "# \t\t# convert image into one sample\n",
    "# \t\tsample = np.expand_dims(scaled_image, 0)\n",
    "# \t\t# make prediction\n",
    "# \t\tyhat = model.detect(sample, verbose=0)\n",
    "# \t\t# extract results for first sample\n",
    "# \t\tr = yhat[0]\n",
    "# \t\t# calculate statistics, including AP\n",
    "# \t\tAP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "# \t\t# store\n",
    "# \t\tAPs.append(AP)\n",
    "# \t# calculate the mean AP across all images\n",
    "# \tmAP = mean(APs)\n",
    "# \treturn mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MaskRCNN(mode='inference', model_dir='./', config=config)\n",
    "# model.load_weights('mask_rcnn_rice_config_0005.h5', by_name=True)\n",
    "# train_mAP = evaluate_model(train, model, config)\n",
    "# print('Train mAP:', train_mAP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envrice",
   "language": "python",
   "name": "envrice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
